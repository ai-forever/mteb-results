{
  "dataset_revision": "416b34a802308eac30e4192afc0ff99bb8dcc7f2",
  "evaluation_time": 5.9822821617126465,
  "kg_co2_emissions": 0.0009072195932945074,
  "mteb_version": "1.14.12",
  "scores": {
    "test": [
      {
        "accuracy": 0.234228515625,
        "f1": 0.20342726670301783,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "lrap": 0.3548733181423535,
        "main_score": 0.234228515625,
        "scores_per_experiment": [
          {
            "accuracy": 0.2470703125,
            "f1": 0.18484212826355495,
            "lrap": 0.3579644097222145
          },
          {
            "accuracy": 0.2255859375,
            "f1": 0.21444551710707163,
            "lrap": 0.3510131835937425
          },
          {
            "accuracy": 0.20263671875,
            "f1": 0.165008639599303,
            "lrap": 0.3302001953124926
          },
          {
            "accuracy": 0.23876953125,
            "f1": 0.2157539716201592,
            "lrap": 0.3659396701388814
          },
          {
            "accuracy": 0.22998046875,
            "f1": 0.15738216650997028,
            "lrap": 0.3415662977430481
          },
          {
            "accuracy": 0.24951171875,
            "f1": 0.24222478400092143,
            "lrap": 0.37381998697915886
          },
          {
            "accuracy": 0.23095703125,
            "f1": 0.2331145947356877,
            "lrap": 0.3659735785590202
          },
          {
            "accuracy": 0.23583984375,
            "f1": 0.20111288543157102,
            "lrap": 0.3504638671874924
          },
          {
            "accuracy": 0.23681640625,
            "f1": 0.21426157600738704,
            "lrap": 0.35652669270832577
          },
          {
            "accuracy": 0.2451171875,
            "f1": 0.20612640375455182,
            "lrap": 0.35526529947915914
          }
        ]
      }
    ]
  },
  "task_name": "SensitiveTopicsClassification"
}